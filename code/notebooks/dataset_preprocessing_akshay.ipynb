{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "\n",
    "\n",
    "def make_gaussian(size, fwhm=125, center=None):\n",
    "    \"\"\"\n",
    "    Make a square gaussian kernel. The code comes from here: https://gist.github.com/andrewgiessel/4635563.\n",
    "    Usage:\n",
    "     gauss = make_gaussian(size)\n",
    "    Parameters\n",
    "    ----------\n",
    "    size: int\n",
    "      Length of a side of the square.\n",
    "    fwhm: int\n",
    "      Full-width-half-maximum, which can be thought of as an effective radius.\n",
    "    center: tuple\n",
    "      Position of the center of the gaussian, default is at the center of the image.\n",
    "    Returns\n",
    "    -------\n",
    "    image: ndarray, shape (width, height)\n",
    "      An image that contains a 2D Gaussian\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.arange(0, size, 1, float)\n",
    "    y = x[:, np.newaxis]\n",
    "\n",
    "    if center is None:\n",
    "        x0 = y0 = size // 2\n",
    "    else:\n",
    "        x0 = center[0]\n",
    "        y0 = center[1]\n",
    "\n",
    "    return np.exp(-4 * np.log(2) * ((x - x0) ** 2 + (y - y0) ** 2) / fwhm ** 2)\n",
    "\n",
    "\n",
    "def preprocessing_unet(im_id,\n",
    "                       mask_bool=True,\n",
    "                       img_dir='./ISIC2018_Task1-2_Training_Input/',\n",
    "                       mask_dir='./ISIC2018_Task1_Training_GroundTruth/',\n",
    "                       img_suffix='.jpg',\n",
    "                       mask_suffix='_segmentation.png',\n",
    "                       largest_dimension=250,\n",
    "                       desired_size=320):\n",
    "    \"\"\"\n",
    "    From a RGB image, create a 5-channel image that contains :\n",
    "        - RGB channels after a histogram equalization has been done on the intensity channel in the HSI space,\n",
    "        - the original intensity channel,\n",
    "        - a 2D gaussian centered on the image.\n",
    "    Besides, we resize the image following the method indicated by the paper.\n",
    "    Usage:\n",
    "     im, mask = preprocessing_unet(im_id) # if training set\n",
    "     im = preprocessing_unet(im_id, mask=False) # if test set\n",
    "    Parameters\n",
    "    ----------\n",
    "    im_id: string\n",
    "      Id of the image.\n",
    "    mask_bool: boolean\n",
    "      Indicates if there is a mask to process (e.g. for the training set), default is True.\n",
    "    img_dir: string\n",
    "      Folder that contains the original images.\n",
    "    mask_dir: string\n",
    "      Folder that contains the ground truth masks.\n",
    "    img_suffix: string\n",
    "      Suffix for the image, default is .jpg.\n",
    "    mask_suffix: string\n",
    "      Suffix for the mask, default is _segmentation.png.\n",
    "    largest_dimension: int\n",
    "      The largest dimension of the image before padding.\n",
    "    desired_size: int\n",
    "      Pad the image so it is a square image whose dimensions have the desired size.\n",
    "    Returns\n",
    "    -------\n",
    "    im_5ch: ndarray, shape (width, height, channels)\n",
    "      The preprocessed image.\n",
    "    new_mask: ndarray, shape (width, height) if mask_bool is True\n",
    "      The preprocessed mask associated to im_5ch.\n",
    "    \"\"\"\n",
    "\n",
    "    new_im = imread(img_dir + im_id + img_suffix)\n",
    "    if mask_bool:\n",
    "        new_mask = imread(mask_dir + im_id + mask_suffix)\n",
    "\n",
    "    # resize so that the largest dimension is 250\n",
    "    rows, columns, _ = new_im.shape\n",
    "\n",
    "    if rows >= columns:\n",
    "        percent = largest_dimension / float(rows)\n",
    "        csize = int((float(columns) * float(percent)))\n",
    "        new_im = cv.resize(new_im, (csize, largest_dimension))\n",
    "        if mask_bool:\n",
    "            new_mask = cv.resize(new_mask, (csize, largest_dimension))\n",
    "\n",
    "    else:\n",
    "        percent = largest_dimension / float(columns)\n",
    "        rsize = int((float(rows) * float(percent)))\n",
    "        new_im = cv.resize(new_im, (largest_dimension, rsize))\n",
    "        if mask_bool:\n",
    "            new_mask = cv.resize(new_mask, (largest_dimension, rsize))\n",
    "\n",
    "    # convert RGB image to HSI image\n",
    "    im_hsi = cv.cvtColor(new_im, cv.COLOR_RGB2HLS)\n",
    "\n",
    "    # original intensity channel\n",
    "    original_intensity = im_hsi[:, :, 1]\n",
    "\n",
    "    delta_w = desired_size - new_im.shape[1]\n",
    "    delta_h = desired_size - new_im.shape[0]\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "    # we pad this image\n",
    "    original_intensity = cv.copyMakeBorder(original_intensity, top, bottom, left, right, cv.BORDER_CONSTANT,\n",
    "                                           value=[255])\n",
    "    original_intensity = np.expand_dims(original_intensity, axis=2)\n",
    "\n",
    "    # histogram equalization on the intensity channel then convert back to RGB\n",
    "    im_hsi[:, :, 1] = cv.equalizeHist(im_hsi[:, :, 1])\n",
    "    new_im = cv.cvtColor(im_hsi, cv.COLOR_HLS2RGB)\n",
    "\n",
    "    # we pad the image\n",
    "    new_im = cv.copyMakeBorder(new_im, top, bottom, left, right, cv.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "\n",
    "    if mask_bool:\n",
    "        # we also pad the mask\n",
    "        new_mask = cv.copyMakeBorder(new_mask, top, bottom, left, right, cv.BORDER_CONSTANT, value=[0])\n",
    "\n",
    "    # 2D gaussian\n",
    "    gauss = make_gaussian(desired_size)\n",
    "    gauss = np.expand_dims(gauss, axis=2)\n",
    "\n",
    "    # concatenation of the different channels\n",
    "    im_5ch = np.concatenate((new_im / 255, original_intensity / 255, gauss), axis=2)\n",
    "\n",
    "    if mask_bool:\n",
    "        return im_5ch, new_mask\n",
    "    else:\n",
    "        return im_5ch\n",
    "\n",
    "\n",
    "# the data set is available here:\n",
    "# https://challenge2018.isic-archive.com/task1/training/\n",
    "\n",
    "def build_training_set(output_path='./ISIC2018_data/',\n",
    "                       img_dir='./ISIC2018_Task1-2_Training_Input/',\n",
    "                       mask_dir='./ISIC2018_Task1_Training_GroundTruth/',\n",
    "                       train_ratio=.8,\n",
    "                       largest_dimension=250,\n",
    "                       desired_size=320,\n",
    "                       img_suffix='.jpg',\n",
    "                       mask_suffix='_segmentation.png',\n",
    "                       specific_ids=['ISIC_0000031', 'ISIC_0000060', 'ISIC_0000073', 'ISIC_0000074', 'ISIC_0000121',\n",
    "                                     'ISIC_0000166', 'ISIC_0000355', 'ISIC_0000395', 'ISIC_0009944', 'ISIC_0010047',\n",
    "                                     'ISIC_0016064'],\n",
    "                       seed=42):\n",
    "    \"\"\"\n",
    "    Build the preprocessed data set from the ISIC data set. The preprocessing applies the method indicated in the paper.\n",
    "    Usage:\n",
    "      Download and unzip the ISIC data set (https://challenge2018.isic-archive.com/task1/training/)\n",
    "      build_training_set()\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_path: string\n",
    "      Folder in which we save the data set.\n",
    "    img_dir: string\n",
    "      Folder that contains the images from ISIC.\n",
    "    mask_dir: string\n",
    "      Folder that contains the masks from ISIC.\n",
    "    train_ratio: float\n",
    "      Proportion of the images in the training set.\n",
    "    largest_dimension: int\n",
    "      The largest dimension of the image before padding.\n",
    "    desired_size: int\n",
    "      Pad the image so it is a square image whose dimensions have the desired size.\n",
    "    img_suffix: string\n",
    "      Extension of the images.\n",
    "    mask_suffix: string\n",
    "      Extension of the masks.\n",
    "    specific_ids: list of strings\n",
    "      Make sure the ids in this list are in the test set.\n",
    "    seed: int\n",
    "      Seed used for the split\n",
    "    \"\"\"\n",
    "\n",
    "    # we create the folders for the data set\n",
    "    if os.path.isdir(output_path) == 0:\n",
    "        os.mkdir(output_path)\n",
    "    if os.path.isdir(output_path + 'train') == 0:\n",
    "        os.mkdir(output_path + 'train')\n",
    "    if os.path.isdir(output_path + 'test') == 0:\n",
    "        os.mkdir(output_path + 'test')\n",
    "\n",
    "    # names of the images\n",
    "    list_ids = os.listdir(img_dir)\n",
    "\n",
    "    # remove the .txt files\n",
    "    if 'LICENSE.txt' in list_ids:\n",
    "        list_ids.remove('LICENSE.txt')\n",
    "    if 'ATTRIBUTION.txt' in list_ids:\n",
    "        list_ids.remove('ATTRIBUTION.txt')\n",
    "\n",
    "    # only keep the ids\n",
    "    for k in range(len(list_ids)):\n",
    "        list_ids[k] = list_ids[k].replace(img_suffix, '')\n",
    "\n",
    "    n = len(list_ids)\n",
    "\n",
    "    # split our data set\n",
    "    indices = np.random.RandomState(seed=seed).permutation(n)\n",
    "    train_idx, validation_idx = indices[:int(train_ratio * n)], indices[int(train_ratio * n):]\n",
    "\n",
    "    partition = {'train': np.array(list_ids)[train_idx],\n",
    "                 'test': np.array(list_ids)[validation_idx]\n",
    "                 }\n",
    "\n",
    "    # check that the ids we want to test are in the test set\n",
    "    if specific_ids:\n",
    "        for k in range(len(partition['train'])):\n",
    "            for id_ in specific_ids:\n",
    "                if id_ == partition['train'][k]:\n",
    "                    rd_idx = np.random.randint(len(partition['test']))\n",
    "                    partition['train'][k] = partition['test'][rd_idx]\n",
    "                    partition['test'][rd_idx] = id_\n",
    "\n",
    "    # create the training set\n",
    "    for k in range(len(partition['train'])):\n",
    "        im_path = partition['train'][k]\n",
    "        im, mask = preprocessing_unet(im_path, True, img_dir, mask_dir, img_suffix, mask_suffix, largest_dimension,\n",
    "                                      desired_size)\n",
    "\n",
    "        hflip_im, hflip_mask = cv.flip(im, 0), cv.flip(mask, 0)\n",
    "        vflip_im, vflip_mask = cv.flip(im, 1), cv.flip(mask, 1)\n",
    "        rot_im, rot_mask = cv.flip(im, -1), cv.flip(mask, -1)\n",
    "\n",
    "        imsave(output_path + 'train/' + partition['train'][k] + '.tiff', im)\n",
    "        imsave(output_path + 'train/' + partition['train'][k] + mask_suffix, mask)\n",
    "        imsave(output_path + 'train/' + 'hflip_' + partition['train'][k] + '.tiff', hflip_im)\n",
    "        imsave(output_path + 'train/' + 'hflip_' + partition['train'][k] + mask_suffix, hflip_mask)\n",
    "        imsave(output_path + 'train/' + 'vflip_' + partition['train'][k] + '.tiff', vflip_im)\n",
    "        imsave(output_path + 'train/' + 'vflip_' + partition['train'][k] + mask_suffix, vflip_mask)\n",
    "        imsave(output_path + 'train/' + 'rot_' + partition['train'][k] + '.tiff', rot_im)\n",
    "        imsave(output_path + 'train/' + 'rot_' + partition['train'][k] + mask_suffix, rot_mask)\n",
    "\n",
    "    # create the test set\n",
    "    for k in range(len(partition['test'])):\n",
    "        im_path = partition['test'][k]\n",
    "        im, mask = preprocessing_unet(im_path, True, img_dir, mask_dir, img_suffix, mask_suffix, largest_dimension,\n",
    "                                      desired_size)\n",
    "\n",
    "        imsave(output_path + 'test/' + partition['test'][k] + '.tiff', im)\n",
    "        imsave(output_path + 'test/' + partition['test'][k] + mask_suffix, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_unet(im_id,\n",
    "                       mask_bool=True,\n",
    "                       img_dir='C:\\Personal\\ISIC2018_Task1-2_Training_Input\\ISIC2018_Task1-2_Training_Input\\\\',\n",
    "                       mask_dir='C:\\Personal\\ISIC2018_Task1_Training_GroundTruth\\ISIC2018_Task1_Training_GroundTruth\\\\',\n",
    "                       img_suffix='.jpg',\n",
    "                       mask_suffix='_segmentation.png',\n",
    "                       largest_dimension=250,\n",
    "                       desired_size=320):\n",
    "    \"\"\"\n",
    "    From a RGB image, create a 5-channel image that contains :\n",
    "        - RGB channels after a histogram equalization has been done on the intensity channel in the HSI space,\n",
    "        - the original intensity channel,\n",
    "        - a 2D gaussian centered on the image.\n",
    "    Besides, we resize the image following the method indicated by the paper.\n",
    "    Usage:\n",
    "     im, mask = preprocessing_unet(im_id) # if training set\n",
    "     im = preprocessing_unet(im_id, mask=False) # if test set\n",
    "    Parameters\n",
    "    ----------\n",
    "    im_id: string\n",
    "      Id of the image.\n",
    "    mask_bool: boolean\n",
    "      Indicates if there is a mask to process (e.g. for the training set), default is True.\n",
    "    img_dir: string\n",
    "      Folder that contains the original images.\n",
    "    mask_dir: string\n",
    "      Folder that contains the ground truth masks.\n",
    "    img_suffix: string\n",
    "      Suffix for the image, default is .jpg.\n",
    "    mask_suffix: string\n",
    "      Suffix for the mask, default is _segmentation.png.\n",
    "    largest_dimension: int\n",
    "      The largest dimension of the image before padding.\n",
    "    desired_size: int\n",
    "      Pad the image so it is a square image whose dimensions have the desired size.\n",
    "    Returns\n",
    "    -------\n",
    "    im_5ch: ndarray, shape (width, height, channels)\n",
    "      The preprocessed image.\n",
    "    new_mask: ndarray, shape (width, height) if mask_bool is True\n",
    "      The preprocessed mask associated to im_5ch.\n",
    "    \"\"\"\n",
    "\n",
    "    new_im = imread(img_dir + im_id + img_suffix)\n",
    "    if mask_bool:\n",
    "        new_mask = imread(mask_dir + im_id + mask_suffix)\n",
    "    print(new_im)\n",
    "    # resize so that the largest dimension is 250\n",
    "    rows, columns, _ = new_im.shape\n",
    "    print(rows,columns)\n",
    "    if rows >= columns:\n",
    "        percent = largest_dimension / float(rows)\n",
    "        csize = int((float(columns) * float(percent)))\n",
    "        new_im = cv.resize(new_im, (csize, largest_dimension))\n",
    "        if mask_bool:\n",
    "            new_mask = cv.resize(new_mask, (csize, largest_dimension))\n",
    "\n",
    "    else:\n",
    "        percent = largest_dimension / float(columns)\n",
    "        rsize = int((float(rows) * float(percent)))\n",
    "        new_im = cv.resize(new_im, (largest_dimension, rsize))\n",
    "        if mask_bool:\n",
    "            new_mask = cv.resize(new_mask, (largest_dimension, rsize))\n",
    "    \n",
    "    # convert RGB image to HSI image\n",
    "    im_hsi = cv.cvtColor(new_im, cv.COLOR_RGB2HLS)\n",
    "\n",
    "    # original intensity channel\n",
    "    original_intensity = im_hsi[:, :, 1]\n",
    "\n",
    "    delta_w = desired_size - new_im.shape[1]\n",
    "    delta_h = desired_size - new_im.shape[0]\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "    # we pad this image\n",
    "    original_intensity = cv.copyMakeBorder(original_intensity, top, bottom, left, right, cv.BORDER_CONSTANT,\n",
    "                                           value=[255])\n",
    "    original_intensity = np.expand_dims(original_intensity, axis=2)\n",
    "\n",
    "    # histogram equalization on the intensity channel then convert back to RGB\n",
    "    im_hsi[:, :, 1] = cv.equalizeHist(im_hsi[:, :, 1])\n",
    "    new_im = cv.cvtColor(im_hsi, cv.COLOR_HLS2RGB)\n",
    "\n",
    "    # we pad the image\n",
    "    new_im = cv.copyMakeBorder(new_im, top, bottom, left, right, cv.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "\n",
    "    if mask_bool:\n",
    "        # we also pad the mask\n",
    "        new_mask = cv.copyMakeBorder(new_mask, top, bottom, left, right, cv.BORDER_CONSTANT, value=[0])\n",
    "\n",
    "    # 2D gaussian\n",
    "    gauss = make_gaussian(desired_size)\n",
    "    gauss = np.expand_dims(gauss, axis=2)\n",
    "\n",
    "    # concatenation of the different channels\n",
    "    im_5ch = np.concatenate((new_im / 255, original_intensity / 255, gauss), axis=2)\n",
    "    \n",
    "    if mask_bool:\n",
    "        return im_5ch, new_mask\n",
    "    else:\n",
    "        return im_5ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[251 255 255]\n",
      "  [232 241 248]\n",
      "  [243 255 255]\n",
      "  ...\n",
      "  [237 255 255]\n",
      "  [238 250 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[230 234 243]\n",
      "  [175 183 194]\n",
      "  [165 181 197]\n",
      "  ...\n",
      "  [177 203 230]\n",
      "  [184 200 216]\n",
      "  [243 246 251]]\n",
      "\n",
      " [[245 251 255]\n",
      "  [171 179 198]\n",
      "  [147 163 186]\n",
      "  ...\n",
      "  [156 189 222]\n",
      "  [169 190 211]\n",
      "  [243 253 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[238 255 255]\n",
      "  [178 196 234]\n",
      "  [169 188 231]\n",
      "  ...\n",
      "  [186 220 245]\n",
      "  [194 217 235]\n",
      "  [242 254 255]]\n",
      "\n",
      " [[232 244 255]\n",
      "  [183 195 211]\n",
      "  [182 194 216]\n",
      "  ...\n",
      "  [199 225 242]\n",
      "  [201 217 230]\n",
      "  [244 249 255]]\n",
      "\n",
      " [[252 255 244]\n",
      "  [239 246 238]\n",
      "  [249 255 255]\n",
      "  ...\n",
      "  [238 255 255]\n",
      "  [234 245 251]\n",
      "  [255 255 255]]]\n",
      "767 1022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.13328631e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.19928653e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.26868014e-04],\n",
       "         ...,\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.34161282e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.26868014e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.19928653e-04]],\n",
       " \n",
       "        [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.19928653e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.26913046e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.34256541e-04],\n",
       "         ...,\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.41974554e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.34256541e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.26913046e-04]],\n",
       " \n",
       "        [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.26868014e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.34256541e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.42024948e-04],\n",
       "         ...,\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.50189544e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.42024948e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.34256541e-04]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.34161282e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.41974554e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.50189544e-04],\n",
       "         ...,\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.58823498e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.50189544e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.41974554e-04]],\n",
       " \n",
       "        [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.26868014e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.34256541e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.42024948e-04],\n",
       "         ...,\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.50189544e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.42024948e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.34256541e-04]],\n",
       " \n",
       "        [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.19928653e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.26913046e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.34256541e-04],\n",
       "         ...,\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.41974554e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.34256541e-04],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "          1.26913046e-04]]]),\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_unet('ISIC_0000000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
